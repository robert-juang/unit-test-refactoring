{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from errors import * \n",
    "from constants import IGNORED_FILE\n",
    "from helper import * \n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "def write_file(path, filename, content, type=\"test\"):  \n",
    "    \"\"\"\n",
    "    Given path, filename, and content, write to file\n",
    "\n",
    "    Ignore file if in IGNORED_FILE defined in constants\n",
    "\n",
    "    type can be test or refactor\n",
    "    \"\"\"\n",
    "    try: \n",
    "        print(f\"Writing test for: \", filename) if type == \"test\" else print(f\"Refactoring code for: \", filename)\n",
    "\n",
    "        with open(path, \"w+\", encoding=\"utf-8\") as f: \n",
    "            f.write(content) \n",
    "    except Exception as e: \n",
    "        print(\"Error when writing file with path: \", path) \n",
    "        print(e) \n",
    "    \n",
    "def write_json_file(path, content): \n",
    "    if not path.endswith(\".json\"): \n",
    "        return None \n",
    "    \n",
    "    filename = os.path.basepath(path) \n",
    "\n",
    "    print('Writing, ', filename)\n",
    "\n",
    "    with open(f\"test_{filename}\", \"w+\", encoding=\"utf-8\") as f: \n",
    "        json.dump(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import re \n",
    "\n",
    "ignored_file = IGNORED_FILE \n",
    "\n",
    "class FileNode: \n",
    "    def __init__(self, path: str, \n",
    "                 source_dir: str=\"src\"):\n",
    "        if (not os.path.isfile(path)): \n",
    "            raise FileExistsError\n",
    "        self.type: str = \"file\" \n",
    "        self.path: str = path \n",
    "        self.name: str = os.path.basename(path) \n",
    "        self.unit_test_path: str = self._get_unit_test_path(self.path, self.name, source_dir) \n",
    "        self.unit_test_path_without_file: str = os.path.dirname(self.unit_test_path) \n",
    "        self.size: str = \"\" #TODO: use module to get \n",
    "        self.file_content_modified: str = \"\"\n",
    "        self.past_file_content: list[str] = [] #tracks past version of unit test in case we want to revert (maybe use dict bc i think we should only use 5 so we don't have ridiculous memory)\n",
    "\n",
    "        try: \n",
    "            with open(path, \"r\") as f: \n",
    "                self.file_content_original: str = f.read() \n",
    "        except Exception as e: \n",
    "            logging.error(\"Error when reading file: \",e) \n",
    "            self.file_content_original = \"\"\n",
    "\n",
    "    def _get_unit_test_path(self, \n",
    "                            current_path: str, \n",
    "                            file_name: str, \n",
    "                            source_dir: str): \n",
    "        \"\"\"Get directory, replace src with test, then attach test filename\n",
    "        \n",
    "        BUG: Might cause an issue with multiple src folders being replaced\n",
    "        \"\"\"\n",
    "        intermediate_path = os.path.dirname(current_path) \n",
    "\n",
    "        intermediate_path = intermediate_path.replace(source_dir, \"test\")\n",
    "        \n",
    "        return os.path.join(intermediate_path, f\"test_{file_name}\")\n",
    "    \n",
    "    def _postprocess(self, gpt_result): \n",
    "        \"\"\"TODO: edit as see fit\"\"\"\n",
    "        return gpt_result \n",
    "\n",
    "    def unit_test(self): \n",
    "        \"\"\" Run unit test in the file. Return the unit tested code\n",
    "        TODO: probbaly use could use some code from aisuite \n",
    "        \"\"\" \n",
    "        \n",
    "        query = \"...\" \n",
    "\n",
    "        #unit_test = await self.gpt_module(query=query) \n",
    "        \n",
    "        unit_test = \"print('hi')\" \n",
    "\n",
    "        unit_test = self._postprocess(unit_test) \n",
    "\n",
    "        return unit_test \n",
    "\n",
    "\n",
    "    def refactor(self): \n",
    "        \"\"\"Refactor the code in this file. Return the refactored code\n",
    "        \"\"\"\n",
    "        query = \"...\" \n",
    "\n",
    "        #unit_test = await self.gpt_module(query=query) \n",
    "        refactored = \"\" \n",
    "        \n",
    "        refactored = self._postprocess(refactored) \n",
    "\n",
    "        return refactored \n",
    "\n",
    "class DirectoryNode: \n",
    "    def __init__(self, path: str):\n",
    "        if (not os.path.isdir(path)): \n",
    "            raise DirectoryNodeInitError\n",
    "        self.type: str = \"dir\"\n",
    "        self.path: str = path \n",
    "        self.name: str = os.path.basename(path)\n",
    "        self.directory_content: list[FileNode | DirectoryNode] = []\n",
    "\n",
    "class DirectoryTree: \n",
    "    \"\"\"\n",
    "    Representing a folder using a tree structure \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_path: str): \n",
    "        self.root = self.recursive_generate(DirectoryNode(root_path)) \n",
    "\n",
    "        #TODO: maybe add a cache here for fast retrieval for projects with tons of files \n",
    "\n",
    "    def recursive_generate(self, dir_node: DirectoryNode): \n",
    "        \"\"\" \n",
    "        Recursively populate the directory node\n",
    "\n",
    "        If file append \n",
    "\n",
    "        If directory recurse and populate \n",
    "        \"\"\" \n",
    "        if (not os.path.isdir(dir_node.path)): \n",
    "            raise DirectoryGenerationError\n",
    "\n",
    "        for node in list(os.listdir(dir_node.path)): \n",
    "            path_to_node: str = os.path.join(dir_node.path, node)\n",
    "\n",
    "            if os.path.isfile(path_to_node): \n",
    "                dir_node.directory_content.append(FileNode(path_to_node))\n",
    "                 \n",
    "            if os.path.isdir(path_to_node):\n",
    "                directory_node = DirectoryNode(path_to_node)\n",
    "                directory_content = self.recursive_generate(directory_node)\n",
    "                dir_node.directory_content.append(directory_content)\n",
    "\n",
    "        return dir_node\n",
    "        \n",
    "class DirectoryParser: \n",
    "    def __init__(self, root_path: str): \n",
    "        self.directory_tree: DirectoryTree = DirectoryTree(root_path)\n",
    "        #TODO: get size of directory tree and save that\n",
    "    \n",
    "    #TODO: make this into a decorator\n",
    "    def _crawl_directory(self, \n",
    "                         dir_node: DirectoryNode, \n",
    "                         file_func, \n",
    "                         dir_func, \n",
    "                         dir_level: int = 0): \n",
    "        \"\"\"crawl through all file and print out file or dir name\n",
    "        \n",
    "        Must include file_func and dir_func in case of what to do when encountering different file types\n",
    "        \"\"\"\n",
    "        if (not os.path.isdir(dir_node.path)): \n",
    "            raise DirectoryGenerationError\n",
    "    \n",
    "        for node in dir_node.directory_content: \n",
    "            path_to_node: str = os.path.join(dir_node.path, node.name)\n",
    "            \n",
    "            if os.path.isdir(path_to_node):\n",
    "                dir_func(dir_level, node) \n",
    "                dir_level += 1\n",
    "                self._crawl_directory(dir_node=node, \n",
    "                                      file_func=file_func, \n",
    "                                      dir_func=dir_func, \n",
    "                                      dir_level=dir_level)\n",
    "                dir_level -= 1\n",
    "\n",
    "            if os.path.isfile(path_to_node): #function 2\n",
    "                file_func(dir_level, node)\n",
    "                 \n",
    "    def display_tree(self): \n",
    "        \"\"\"scan through entire tree and display a structure to view\"\"\"\n",
    "        print(self.directory_tree.root.name, \"(root)\")\n",
    "\n",
    "        self._crawl_directory(self.directory_tree.root, \n",
    "                file_func=lambda dir_level, node: print('\\t' * dir_level, '|', node.name),  \n",
    "                dir_func=lambda dir_level, node: print(\"\\t\" * dir_level + \"- \", node.name), \n",
    "        ) \n",
    "    \n",
    "    def retrieve_information(self, node): \n",
    "        \"\"\"Return information about the node\"\"\"\n",
    "\n",
    "        if (node.type == \"dir\"): \n",
    "            return {\n",
    "                \"type\": node.type,\n",
    "                \"filename\": node.name, \n",
    "                \"path\": node.path, \n",
    "                \"directory_content\": node.directory_content\n",
    "            }\n",
    "        elif (node.type == \"file\"): \n",
    "            return {\n",
    "                \"type\": node.type,\n",
    "                \"filename\": node.name, \n",
    "                \"path\": node.path,\n",
    "                \"content\": node.file_content_original, \n",
    "                \"unit_test\": self.file_content_modified, \n",
    "                \"past_unit_test_versions\": self.past_file_content\n",
    "            }\n",
    "\n",
    "    def _retrieve_file(self, dir_node, file_name): \n",
    "        \"\"\"Scan through the entire tree and if filename match return node of file otherwise return None\n",
    "        \n",
    "        TODO: Maybe can optimize with a faster tree ?? Also add error cehcking\n",
    "        \"\"\"\n",
    "\n",
    "        for node in dir_node.directory_content: \n",
    "            path_to_node: str = os.path.join(dir_node.path, node.name)\n",
    "            \n",
    "            if os.path.isdir(path_to_node):\n",
    "                n = self._retrieve_file(node, file_name=file_name)\n",
    "                if n: \n",
    "                    return n\n",
    "            if os.path.isfile(path_to_node) and os.path.basename(path_to_node) == file_name: \n",
    "                return node \n",
    "            \n",
    "        return None \n",
    "    \n",
    "    def retrieve_node_by_filename(self, file_name: str): \n",
    "        \"\"\"Scan thorugh the entire tree and return matching node by file name. If duplicate, raise error\"\"\"\n",
    "        result = self._retrieve_file(self.directory_tree.root, file_name)\n",
    "        \n",
    "        return result if result else None \n",
    "\n",
    "    def _recurse_directory(self, \n",
    "                            root: DirectoryNode, \n",
    "                            ignore_items: list[str]=IGNORED_FILE): \n",
    "        \"\"\"\n",
    "        Generate and write test files for root. Use file_content_original when writing test\n",
    "        \"\"\"\n",
    "        if not root: \n",
    "            raise DirectoryNodeNotExistError\n",
    "\n",
    "        for node in root.directory_content:             \n",
    "            path_to_node: str = os.path.join(root.path, node.name)\n",
    "\n",
    "            print(f\"Ignored {node.name}\") if node.name in IGNORED_FILE else None\n",
    "\n",
    "            if os.path.isdir(path_to_node):\n",
    "                print(f\"||| Opening directory {node.name} |||\")\n",
    "                self._recurse_directory(node, \n",
    "                                        ignore_items)\n",
    "\n",
    "            if os.path.isfile(path_to_node) and node.name not in IGNORED_FILE: \n",
    "                print(f\"Processing {node.type}, \", node.name)\n",
    "\n",
    "                try: \n",
    "                    unit_test = node.unit_test()\n",
    "                except Exception as e: \n",
    "                    print(\"Error when generating unit test: \", e)\n",
    "                    print(\"Skipping file\", node.name)\n",
    "                    continue \n",
    "\n",
    "                os.makedirs(node.unit_test_path_without_file, exist_ok=True)\n",
    "\n",
    "                write_file(path=node.unit_test_path, \n",
    "                           filename=node.name,\n",
    "                           content=unit_test) \n",
    "                \n",
    "    def generate_tests_and_write(self, \n",
    "                                 ignore_items: list[str]=IGNORED_FILE): \n",
    "        \"\"\"Wrapper for recursing directory and generaating tests\"\"\"\n",
    "        try: \n",
    "            self._recurse_directory(root=self.directory_tree.root, \n",
    "                                    ignore_items=ignore_items) \n",
    "        except: \n",
    "            raise UnitTestGenerationError        \n",
    "    #TODO: get different versions to populate shit \n",
    "\n",
    "    def is_test_running(self): \n",
    "        \"\"\" \n",
    "        TODO: tests if all tests are passing automatically \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "\n",
    "    def refactor_existing_test(self, filename): \n",
    "        \"\"\" \n",
    "        Refactor a file in the test folder with the filename specified like \"test_app.py\" \n",
    "        \"\"\"\n",
    "        \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(os.getcwd(), \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_parser = DirectoryParser(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src (root)\n",
      "-  routers\n",
      "\t | router.py\n",
      " | config.py\n",
      "-  utils\n",
      "-  schema\n",
      "\t | serialize.py\n",
      "\t | validate_schema.py\n",
      "\t | output_schema.py\n",
      "\t | input_schema.py\n",
      " | app.py\n",
      " | errors.py\n",
      " | main.py\n",
      "-  gpt_model\n",
      "\t | generate_authorization_token.py\n",
      "\t | call_retry.py\n",
      "\t | response_handler.py\n",
      "\t | gpt_extraction.py\n",
      "\t | prompts.py\n",
      "\t | gpt_confidence_score.py\n",
      "\t | gpt.py\n"
     ]
    }
   ],
   "source": [
    "directory_parser.display_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = directory_parser.retrieve_node_by_filename(\"response_handler.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FileNode at 0x10b699f10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'file',\n",
       " 'path': '/Users/zhidongjiang/Desktop/unit-test-refactoring/src/gpt_model/response_handler.py',\n",
       " 'name': 'response_handler.py',\n",
       " 'unit_test_path': '/Users/zhidongjiang/Desktop/unit-test-refactoring/test/gpt_model/test_response_handler.py',\n",
       " 'unit_test_path_without_file': '/Users/zhidongjiang/Desktop/unit-test-refactoring/test/gpt_model',\n",
       " 'size': '',\n",
       " 'file_content_modified': '',\n",
       " 'past_file_content': [],\n",
       " 'file_content_original': 'import aiohttp \\nimport json \\nimport re \\nimport requests\\nimport uuid \\n\\nfrom config import logger, settings\\nfrom errors import UnknownGPTError\\n\\nfrom collections import defaultdict\\nfrom fastapi import status \\nfrom typing import List, Dict, Tuple, Any \\nimport numpy as np\\n\\nclass GPTResponseHandler(): \\n    def __init__(self):\\n        self.session_id = str(uuid.uuid4())\\n        self.gpt_version = settings.GPT_VERSION\\n\\n    def _get_payload(self, req_id, user_message, system_message=None): \\n        messages = [{\"role\": \"user\", \"content\": user_message}]\\n\\n        if system_message: \\n            messages.insert(0, {\"role\": \"system\", \"content\": system_message})\\n\\n        payload = {\\n            \\'session_id\\': req_id, \\n            \"messages\": messages, \\n            \"n\": 1,\\n            \"frequency_penalty\": 0,\\n            \"presence_penalty\": 0,\\n            \"temperature\": 0, \\n            \"top_p\": 1, \\n            \"stop\": None,\\n        }\\n\\n\\n\\n        return payload\\n    \\n    def fix_gpt_response(self, input_response: str): \\n        return input_response\\n    \\n    def _succeed_response(self, input_response: str):\\n        response = self.fix_gpt_response(input_response)\\n\\n        response_content = response[\"choices\"][0][\"message\"][\"content\"]\\n\\n        json_response = response_content.replace(\\'<json>\\', \\'\\').replace(\\'</json>\\', \\'\\') \\n\\n        try: \\n            json_response = json.loads(json_response) \\n        except json.JSONDecodeError as e:\\n            json_response = self._fix_json_error(json_response)\\n            json_response = json.loads(json_response)\\n        \\n        return response, json_response \\n    \\n    @retry_call(exception=(GPTExtractionError), max_retries=settings.MAX_RETRIES)\\n    async def gpt_call(self,\\n                       url: str, \\n                       user_message: str, \\n                       system_message: str, \\n                       req_id: str=\"\"): \\n        url = url.replace(\"__req_id__\", req_id) \\n        payload = self._get_payload(req_id, user_message, system_message)\\n\\n        logger.info(f\"Requesting GPT with payload: {payload}\")\\n\\n        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=15, ssl=False)) as session:\\n            async with session.post(url, headers=self._get_headers(), data = json.dumps(payload), verify_ssl=False) as response:\\n                status_code = response.status\\n                response_text = await response.text()\\n\\n                response = await response.json()\\n                logger.info(f\"Response received from GPT: {response}\")\\n                \\n                if status_code == 200:\\n                    response_json = await response.json() \\n                    response_data, json_response = self._succeed_response(response_json)\\n\\n                    return {\\n                        \"status_code\": status_code, \\n                        \"response_text\": response_text,\\n                        \"response_data\": response_data, \\n                        \"json_response\": json_response\\n                    } \\n                \\n                elif status_code == 429: \\n                    logger.error(f\"Rate limit error: {response_text}\")\\n                    raise GPTRateLimitError(response_text) \\n                \\n'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||| Opening directory routers |||\n",
      "Processing file,  router.py\n",
      "Writing test for:  router.py\n",
      "Processing file,  config.py\n",
      "Writing test for:  config.py\n",
      "||| Opening directory utils |||\n",
      "||| Opening directory schema |||\n",
      "Processing file,  serialize.py\n",
      "Writing test for:  serialize.py\n",
      "Processing file,  validate_schema.py\n",
      "Writing test for:  validate_schema.py\n",
      "Processing file,  output_schema.py\n",
      "Writing test for:  output_schema.py\n",
      "Processing file,  input_schema.py\n",
      "Writing test for:  input_schema.py\n",
      "Processing file,  app.py\n",
      "Writing test for:  app.py\n",
      "Processing file,  errors.py\n",
      "Writing test for:  errors.py\n",
      "Processing file,  main.py\n",
      "Writing test for:  main.py\n",
      "||| Opening directory gpt_model |||\n",
      "Processing file,  generate_authorization_token.py\n",
      "Writing test for:  generate_authorization_token.py\n",
      "Processing file,  call_retry.py\n",
      "Writing test for:  call_retry.py\n",
      "Processing file,  response_handler.py\n",
      "Writing test for:  response_handler.py\n",
      "Processing file,  gpt_extraction.py\n",
      "Writing test for:  gpt_extraction.py\n",
      "Processing file,  prompts.py\n",
      "Writing test for:  prompts.py\n",
      "Processing file,  gpt_confidence_score.py\n",
      "Writing test for:  gpt_confidence_score.py\n",
      "Processing file,  gpt.py\n",
      "Writing test for:  gpt.py\n"
     ]
    }
   ],
   "source": [
    "directory_parser.generate_tests_and_write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
